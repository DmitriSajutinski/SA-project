{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2374,
     "status": "ok",
     "timestamp": 1639418327401,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "vwt63CFchwUN",
    "outputId": "535e8596-9171-45fa-f137-58bf08f14fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# GD-ga Ã¼hendamine\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1639418327404,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "hoNvJ_FKii_R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Keras\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Input, BatchNormalization, LayerNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1639418327845,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "Xb7vYLU0iEG5"
   },
   "outputs": [],
   "source": [
    "failinimi = 'boardgames1.csv'\n",
    "mangud = pd.read_csv(failinimi, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1639418327846,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "rAfVPCgfinP0",
    "outputId": "9bdfd663-0e03-415e-b8d5-6cfe9f7a4eb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectid</th>\n",
       "      <th>yearpublished</th>\n",
       "      <th>sortindex</th>\n",
       "      <th>minplayers</th>\n",
       "      <th>maxplayers</th>\n",
       "      <th>minplaytime</th>\n",
       "      <th>maxplaytime</th>\n",
       "      <th>minage</th>\n",
       "      <th>min_community</th>\n",
       "      <th>max_community</th>\n",
       "      <th>totalvotes</th>\n",
       "      <th>languagedependence</th>\n",
       "      <th>usersrated</th>\n",
       "      <th>average</th>\n",
       "      <th>baverage</th>\n",
       "      <th>stddev</th>\n",
       "      <th>avgweight</th>\n",
       "      <th>numweights</th>\n",
       "      <th>numgeeklists</th>\n",
       "      <th>numtrading</th>\n",
       "      <th>numwanting</th>\n",
       "      <th>numcomments</th>\n",
       "      <th>siteviews</th>\n",
       "      <th>numplays</th>\n",
       "      <th>numplays_month</th>\n",
       "      <th>news</th>\n",
       "      <th>blogs</th>\n",
       "      <th>weblink</th>\n",
       "      <th>podcast</th>\n",
       "      <th>boardgamedesigner_cnt</th>\n",
       "      <th>boardgameartist_cnt</th>\n",
       "      <th>boardgamepublisher_cnt</th>\n",
       "      <th>boardgamehonor_cnt</th>\n",
       "      <th>boardgamecategory_cnt</th>\n",
       "      <th>boardgamemechanic_cnt</th>\n",
       "      <th>boardgameexpansion_cnt</th>\n",
       "      <th>boardgameversion_cnt</th>\n",
       "      <th>boardgamefamily_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>14141.000000</td>\n",
       "      <td>14803.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>88667.61500</td>\n",
       "      <td>1981.268700</td>\n",
       "      <td>10000.500000</td>\n",
       "      <td>2.055250</td>\n",
       "      <td>5.59215</td>\n",
       "      <td>68.096450</td>\n",
       "      <td>94.28945</td>\n",
       "      <td>9.476350</td>\n",
       "      <td>3.203522</td>\n",
       "      <td>4.277917</td>\n",
       "      <td>13.461350</td>\n",
       "      <td>1.35465</td>\n",
       "      <td>739.616350</td>\n",
       "      <td>6.275440</td>\n",
       "      <td>5.297689</td>\n",
       "      <td>1.499691</td>\n",
       "      <td>1.931761</td>\n",
       "      <td>48.059050</td>\n",
       "      <td>244.243000</td>\n",
       "      <td>40.097900</td>\n",
       "      <td>39.376050</td>\n",
       "      <td>191.515100</td>\n",
       "      <td>7.632353e+04</td>\n",
       "      <td>2304.163050</td>\n",
       "      <td>16.018050</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>10.146900</td>\n",
       "      <td>6.255150</td>\n",
       "      <td>1.943050</td>\n",
       "      <td>1.339200</td>\n",
       "      <td>1.379250</td>\n",
       "      <td>2.500100</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>2.568050</td>\n",
       "      <td>2.306900</td>\n",
       "      <td>1.185750</td>\n",
       "      <td>3.392800</td>\n",
       "      <td>1.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>90640.91959</td>\n",
       "      <td>219.223277</td>\n",
       "      <td>5773.647028</td>\n",
       "      <td>0.745537</td>\n",
       "      <td>15.04921</td>\n",
       "      <td>466.502106</td>\n",
       "      <td>1005.75196</td>\n",
       "      <td>3.738842</td>\n",
       "      <td>1.398935</td>\n",
       "      <td>2.155624</td>\n",
       "      <td>57.169377</td>\n",
       "      <td>1.39713</td>\n",
       "      <td>3096.843206</td>\n",
       "      <td>1.065339</td>\n",
       "      <td>1.494174</td>\n",
       "      <td>0.340743</td>\n",
       "      <td>0.897206</td>\n",
       "      <td>198.113638</td>\n",
       "      <td>1079.125263</td>\n",
       "      <td>94.094807</td>\n",
       "      <td>113.503485</td>\n",
       "      <td>606.892077</td>\n",
       "      <td>2.252393e+05</td>\n",
       "      <td>13625.576506</td>\n",
       "      <td>111.374578</td>\n",
       "      <td>1.282754</td>\n",
       "      <td>38.772952</td>\n",
       "      <td>11.053866</td>\n",
       "      <td>7.956222</td>\n",
       "      <td>0.787003</td>\n",
       "      <td>4.768926</td>\n",
       "      <td>5.082961</td>\n",
       "      <td>1.609824</td>\n",
       "      <td>1.366515</td>\n",
       "      <td>1.707764</td>\n",
       "      <td>6.809425</td>\n",
       "      <td>12.317236</td>\n",
       "      <td>1.728375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-3500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.830000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5858.75000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>5000.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>5.712777</td>\n",
       "      <td>5.502933</td>\n",
       "      <td>1.309920</td>\n",
       "      <td>1.255050</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.163450e+04</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39278.50000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>10000.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>6.354170</td>\n",
       "      <td>5.545040</td>\n",
       "      <td>1.471575</td>\n",
       "      <td>1.910900</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>2.366050e+04</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>169680.50000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>15000.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>90.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>333.250000</td>\n",
       "      <td>6.955793</td>\n",
       "      <td>5.674960</td>\n",
       "      <td>1.663783</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>5.791975e+04</td>\n",
       "      <td>703.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>295770.00000</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>120000.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1868.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>90730.000000</td>\n",
       "      <td>9.442860</td>\n",
       "      <td>8.584240</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7104.000000</td>\n",
       "      <td>121243.000000</td>\n",
       "      <td>2311.000000</td>\n",
       "      <td>2068.000000</td>\n",
       "      <td>17143.000000</td>\n",
       "      <td>8.933078e+06</td>\n",
       "      <td>627501.000000</td>\n",
       "      <td>5694.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1158.000000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           objectid  yearpublished  ...  boardgameversion_cnt  boardgamefamily_cnt\n",
       "count   20000.00000   20000.000000  ...          20000.000000         20000.000000\n",
       "mean    88667.61500    1981.268700  ...              3.392800             1.429500\n",
       "std     90640.91959     219.223277  ...             12.317236             1.728375\n",
       "min         1.00000   -3500.000000  ...              0.000000             0.000000\n",
       "25%      5858.75000    1997.000000  ...              1.000000             0.000000\n",
       "50%     39278.50000    2008.000000  ...              2.000000             1.000000\n",
       "75%    169680.50000    2015.000000  ...              3.000000             2.000000\n",
       "max    295770.00000    2021.000000  ...           1002.000000            86.000000\n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mangud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1639418327848,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "265CTpBGjM2T",
    "outputId": "4d5665f8-8833-4032-ef9b-236364df7cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19905 entries, 0 to 19999\n",
      "Data columns (total 32 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   minplayers              19905 non-null  int64  \n",
      " 1   maxplayers              19905 non-null  int64  \n",
      " 2   minplaytime             19905 non-null  int64  \n",
      " 3   maxplaytime             19905 non-null  int64  \n",
      " 4   minage                  19905 non-null  int64  \n",
      " 5   languagedependence      19905 non-null  int64  \n",
      " 6   usersrated              19905 non-null  int64  \n",
      " 7   average                 19905 non-null  float64\n",
      " 8   baverage                19905 non-null  float64\n",
      " 9   stddev                  19905 non-null  float64\n",
      " 10  avgweight               19905 non-null  float64\n",
      " 11  numweights              19905 non-null  int64  \n",
      " 12  numgeeklists            19905 non-null  int64  \n",
      " 13  numtrading              19905 non-null  int64  \n",
      " 14  numwanting              19905 non-null  int64  \n",
      " 15  numcomments             19905 non-null  int64  \n",
      " 16  siteviews               19905 non-null  int64  \n",
      " 17  numplays                19905 non-null  int64  \n",
      " 18  numplays_month          19905 non-null  int64  \n",
      " 19  news                    19905 non-null  int64  \n",
      " 20  blogs                   19905 non-null  int64  \n",
      " 21  weblink                 19905 non-null  int64  \n",
      " 22  podcast                 19905 non-null  int64  \n",
      " 23  boardgamedesigner_cnt   19905 non-null  int64  \n",
      " 24  boardgameartist_cnt     19905 non-null  int64  \n",
      " 25  boardgamepublisher_cnt  19905 non-null  int64  \n",
      " 26  boardgamehonor_cnt      19905 non-null  int64  \n",
      " 27  boardgamecategory_cnt   19905 non-null  int64  \n",
      " 28  boardgamemechanic_cnt   19905 non-null  int64  \n",
      " 29  boardgameexpansion_cnt  19905 non-null  int64  \n",
      " 30  boardgameversion_cnt    19905 non-null  int64  \n",
      " 31  boardgamefamily_cnt     19905 non-null  int64  \n",
      "dtypes: float64(4), int64(28)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "def drop_bad_columns(df):\n",
    "    # Tugevalt varieeruvad, nullvÃ¤Ã¤rtustega, vÃ¤ga hÃµredate andmetega jms puudustega tunnused,\n",
    "    # eemaldame need kÃµik ja tagastame andmestiku kus ei ole mÃ¼ra tekitavaid tunnuseid\n",
    "    bad_columns = ['yearpublished', 'objectid', 'name', 'min_community', 'max_community', 'label',\n",
    "                   'playerage', 'totalvotes', 'sortindex', 'boardgamedesigner',\n",
    "                   'boardgamehonor', 'boardgamecategory', 'boardgamemechanic', \n",
    "                   'boardgameexpansion', 'boardgamefamily', 'description', 'gamelink',\n",
    "                   'boardgameartist', 'boardgamepublisher', 'boardgameversion']\n",
    "    df = df[df['usersrated'] > 0]\n",
    "    return df.drop(columns=bad_columns)\n",
    "mangud = drop_bad_columns(mangud)\n",
    "mangud.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1639418327849,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "0YIC-5gCkDzs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1639418328167,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "OUr30a8jlGW_",
    "outputId": "4435618f-7886-4e21-dfd8-263999a5217c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16122, 31) (16122, 1)\n",
      "(1991, 31) (1991, 1)\n",
      "(1792, 31) (1792, 1)\n"
     ]
    }
   ],
   "source": [
    "#pipe = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X = mangud.drop('average', 1)\n",
    "y = mangud['average'].values.reshape(-1,1)\n",
    "y = y_scaler.fit_transform(y)\n",
    "\n",
    "applicable = list(X.columns)\n",
    "X[applicable] = X_scaler.fit_transform(X[applicable])\n",
    "\n",
    "\n",
    "\n",
    "#mangud.describe()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1639418328168,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "AzMj2XrewKad"
   },
   "outputs": [],
   "source": [
    "mdl = Sequential()\n",
    "\n",
    "mdl.add(Dense(128, activation=\"selu\", input_shape=(X.shape[1],)))\n",
    "mdl.add(LayerNormalization())\n",
    "mdl.add(Dense(64, activation=\"relu\"))\n",
    "mdl.add(LayerNormalization())\n",
    "mdl.add(Dense(16, activation=\"elu\"))\n",
    "mdl.add(LayerNormalization())\n",
    "mdl.add(Dense(8, activation=\"elu\"))\n",
    "mdl.add(LayerNormalization())\n",
    "mdl.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "mdl.compile(loss='mean_squared_error', optimizer='adamax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1763821,
     "status": "ok",
     "timestamp": 1639420091980,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "k1zWGJEAwhbF",
    "outputId": "a069476b-b0eb-4756-ee55-97e9ac87446a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "1008/1008 [==============================] - 5s 3ms/step - loss: 0.0091 - val_loss: 0.0075\n",
      "Epoch 2/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 3/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 4/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 5/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 6/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0058 - val_loss: 0.0059\n",
      "Epoch 7/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 8/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 9/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 10/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 11/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0051 - val_loss: 0.0053\n",
      "Epoch 12/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 13/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 14/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 15/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0046 - val_loss: 0.0048\n",
      "Epoch 16/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 17/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 18/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 19/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 20/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 21/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 22/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 23/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 24/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 25/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 26/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 27/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 28/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 29/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 30/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 31/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 32/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 33/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 34/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 35/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 36/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 37/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 38/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 39/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 40/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 41/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 42/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 43/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 44/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 45/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0031 - val_loss: 0.0035\n",
      "Epoch 46/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 47/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0031 - val_loss: 0.0057\n",
      "Epoch 48/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 49/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 50/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 51/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 52/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 53/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 54/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 55/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 56/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 57/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0030 - val_loss: 0.0058\n",
      "Epoch 58/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 59/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 60/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 61/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 62/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 63/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 64/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 65/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 66/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 67/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 68/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 69/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 70/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 71/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 72/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 73/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 74/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 75/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 76/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 77/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 78/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 79/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 80/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0026 - val_loss: 0.0040\n",
      "Epoch 81/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 82/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 83/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 84/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 85/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 86/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 87/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 88/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 89/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 90/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 91/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 92/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 93/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 94/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 95/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 96/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 97/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 98/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 99/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 100/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 101/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 102/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 103/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 104/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 105/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 106/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 107/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 108/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 109/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0041\n",
      "Epoch 110/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 111/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 112/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 113/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 114/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 115/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 116/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 117/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0029\n",
      "Epoch 118/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 119/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 120/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 121/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 122/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 123/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 124/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 125/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 126/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 127/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 128/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 129/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 130/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 131/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 132/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 133/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 134/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 135/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 136/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 137/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 138/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 139/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 140/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 141/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 142/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 143/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 144/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 145/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 146/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 147/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 148/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 149/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 150/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 151/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 152/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 153/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 154/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 155/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 156/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 157/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 158/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 159/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 160/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 161/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0059\n",
      "Epoch 162/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 163/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 164/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 165/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 166/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 167/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 168/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 169/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 170/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 171/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 172/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 173/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 174/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 175/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 176/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 177/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 178/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 179/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 180/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 181/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 182/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 183/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 184/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 185/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 186/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 187/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 188/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 189/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 190/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 191/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 192/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 193/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 194/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 195/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 196/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 197/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 198/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 199/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 200/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 201/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 202/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 203/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 204/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 205/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 206/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 207/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 208/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 209/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 210/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 211/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 212/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 213/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 214/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 215/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 216/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 217/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 218/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 219/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 220/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 221/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 222/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 223/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 224/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 225/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 226/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 227/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 228/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 229/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 230/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 231/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 232/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 233/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 234/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 235/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 236/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 237/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 238/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 239/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 240/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 241/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 242/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 243/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 244/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 245/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 246/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 247/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 248/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 249/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 250/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 251/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 252/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 253/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 254/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 255/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 256/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 257/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 258/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 259/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 260/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 261/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 262/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 263/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 264/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 265/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 266/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 267/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 268/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 269/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 270/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 271/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 272/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 273/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 274/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 275/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 276/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 277/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 278/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 279/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 280/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 281/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 282/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 283/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 284/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 285/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 286/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 287/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 288/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 289/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 290/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 291/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 292/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 293/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 294/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 295/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 296/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 297/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 298/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 299/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 300/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 301/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 302/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 303/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 304/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 305/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0043\n",
      "Epoch 306/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 307/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 308/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 309/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 310/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 311/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 312/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 313/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 314/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 315/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 316/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 317/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 318/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 319/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 320/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 321/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 322/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 323/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 324/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 325/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 326/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 327/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 328/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 329/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 330/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 331/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 332/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 333/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 334/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 335/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 336/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 337/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 338/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 339/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 340/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 341/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 342/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 343/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 344/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 345/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 346/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 347/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 348/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 349/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 350/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 351/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 352/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 353/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 354/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 355/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 356/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 357/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 358/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 359/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 360/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 361/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 362/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 363/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 364/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 365/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 366/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 367/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 368/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 369/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 370/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 371/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 372/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 373/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 374/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 375/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 376/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 377/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 378/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 379/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 380/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 381/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 382/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 383/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 384/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 385/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 386/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 387/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 388/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 389/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 390/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 391/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 392/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 393/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 394/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 395/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 396/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 397/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 398/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 399/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 400/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 401/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 402/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 403/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 404/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 405/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 406/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 407/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 408/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 409/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 410/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 411/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 412/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 413/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 414/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 415/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 416/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 417/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 418/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 419/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 420/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 421/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 422/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 423/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 424/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 425/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 426/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 427/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 428/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 429/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 430/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 431/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 432/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 433/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 434/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 435/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 436/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 437/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 438/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 439/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 440/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 441/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 442/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 443/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 444/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 445/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 446/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 447/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 448/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 449/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 450/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 451/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 452/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 453/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 454/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 455/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 456/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 457/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 458/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 459/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 460/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 461/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 462/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 463/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 464/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 465/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 466/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 467/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 468/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 469/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 470/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 471/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 472/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 473/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 474/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 475/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 476/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 477/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 478/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 479/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 480/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 481/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 482/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 483/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 484/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 485/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 486/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 487/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 488/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 489/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 490/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 491/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 492/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 493/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 494/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 495/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 496/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 497/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 498/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 499/512\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 500/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 501/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 502/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 503/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 504/512\n",
      "1008/1008 [==============================] - 4s 3ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 505/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 506/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 507/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 508/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 509/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 510/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 511/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 512/512\n",
      "1008/1008 [==============================] - 4s 4ms/step - loss: 0.0013 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "#callback = EarlyStopping(patience=10)\n",
    "hist = mdl.fit(X_train, y_train, validation_data = (X_val, y_val), \n",
    "                   epochs=512, batch_size=16, verbose=True)#, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1639420091980,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "o1Hyz9v9xCpN",
    "outputId": "70c5bee3-1857-4de6-eeff-78c62b883fbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f874d163410>]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdl.save('mangud.h5')\n",
    "plt.plot(hist.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1639420092377,
     "user": {
      "displayName": "Joosep Tavits",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09788277267329987190"
     },
     "user_tz": -120
    },
    "id": "H8KuSGQkxRLl",
    "outputId": "ac8b2f17-1912-4b86-acb2-782f83bc88e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normaliseerimata andmetel ennustuste keskmine absoluutviga:  0.195442968154209\n",
      "\n",
      "Ennustatud vÃ¤Ã¤rtus vs Ãµige vÃ¤Ã¤rtus (Normaliseeritud)\n",
      "\n",
      "[0.5022747] [0.22109451]\n",
      "[0.6703428] [0.66598167]\n",
      "[0.6307224] [0.62782991]\n",
      "[0.4225878] [0.47377311]\n",
      "[0.6328492] [0.66267829]\n",
      "[0.74005985] [0.73773224]\n",
      "[0.60821736] [0.6034602]\n",
      "[0.73438823] [0.73852818]\n",
      "[0.50668466] [0.50296582]\n",
      "[0.6633714] [0.68284325]\n",
      "[0.5938272] [0.62108693]\n",
      "[0.64264685] [0.63406358]\n",
      "[0.77015865] [0.77256167]\n",
      "[0.503193] [0.44276939]\n",
      "[0.5104587] [0.50274315]\n",
      "[0.6103136] [0.60506274]\n",
      "[0.49017742] [0.48690491]\n",
      "[0.6489674] [0.61102754]\n",
      "[0.6298202] [0.60419455]\n",
      "[0.44762695] [0.44352743]\n",
      "\n",
      "Ennustatud vÃ¤Ã¤rtus vs Ãµige vÃ¤Ã¤rtus (Normaliseerimata)\n",
      "\n",
      "[5.240635] [2.86667]\n",
      "[6.6596103] [6.62279]\n",
      "[6.325101] [6.30068]\n",
      "[4.5678496] [5.]\n",
      "[6.343057] [6.5949]\n",
      "[7.2482214] [7.22857]\n",
      "[6.1350937] [6.09493]\n",
      "[7.200337] [7.23529]\n",
      "[5.2778673] [5.24647]\n",
      "[6.6007514] [6.76515]\n",
      "[6.0135994] [6.24375]\n",
      "[6.425777] [6.35331]\n",
      "[7.5023413] [7.52263]\n",
      "[5.248388] [4.73824]\n",
      "[5.309731] [5.24459]\n",
      "[6.152792] [6.10846]\n",
      "[5.1384993] [5.11087]\n",
      "[6.4791408] [6.15882]\n",
      "[6.317484] [6.10113]\n",
      "[4.7792516] [4.74464]\n"
     ]
    }
   ],
   "source": [
    "y_parim = mdl.predict(X_test)\n",
    "real_p = y_scaler.inverse_transform(y_parim)\n",
    "real_t = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "print(\"Normaliseerimata andmetel ennustuste keskmine absoluutviga: \",\n",
    "      mean_absolute_error(real_p, real_t))\n",
    "\n",
    "print()\n",
    "print(\"Ennustatud vÃ¤Ã¤rtus vs Ãµige vÃ¤Ã¤rtus (Normaliseeritud)\")\n",
    "print()\n",
    "for i in range(20):\n",
    "    print(y_parim[i], y_test[i])\n",
    "\n",
    "print()\n",
    "print(\"Ennustatud vÃ¤Ã¤rtus vs Ãµige vÃ¤Ã¤rtus (Normaliseerimata)\")\n",
    "print()\n",
    "for i in range(20):\n",
    "    print(real_p[i], real_t[i])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtnlh+jZJGbQIj0ko7AphL",
   "name": "huinja.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
